# Reinforcement Learning

## Bellman Equation

### 2019. 7. 22. Mon

### CogPsi Study

### YK LEE

### 1. 벨만 방정식

- 가치함수 : 어떤 상태의 가치에 대한 기대

- 어떤 상태의 가치 함수 = 에이젼트가 그 상태로 갈 경우에 앞으로 받을 보상의 합에 대한 기댓값

- 가치함수는 현재 에이젼트의 정책에 영향을 받음 = 벨만 기대 방정식(수식 2.28)

#### 수식 2.28. 벨만 기대 방정식

v<sub>&pi;</sub>(s) = E<sub>/&pi;</sub>[R<sub>t+1</sub> + &gamma;v<sub>&pi;</sub>(S<sub>t+1</sub>|S<sub>t</sub> = s)]

- 벨만 기대 방정식
  - 식에 기댓값의 개념이 들어감
  - 현재 상태의 가치함수와 **다음 상태**의 가치 함수 사이의 관계를 식으로 나타냄
  - 현재 상태에서 이미 가지고있는 v<sub>&pi;</sub>값을 식의 오른쪽에 있는 기댓값으로 업데이트함

#### 수식 2.32. 계산 가능한 벨만 방정식

v<sub>&pi;</sub>(s) = update

- v<sub>k+1</sub>(s) = 현재 정책에 따라 k+1번째 계산한 상태 s의 가치함수
- k+1번째 가치함수는 k번째 가치함수 중에서 주변 상태들 (s')를 이용해 구한다.

상태 변환 확률을 모든 s와 a에 대해 1이라고 가정해본다. 즉, 왼쪽으로 행동을 취하면 왼쪽에 있는 상태로 무조건 가게 되는 환경을 설정.

#### 그림 2.17 예제

A = 현재 에이젼트가 있는 상태
현재 상태의 가치함수 = 0 이라고 가정,
벨만 기대 방정식을 통해 업데이트한 가치함수 값은?

- action space = {left, right, up, down}
  <초기 가정>
- 초기 정책 : 무작위로 행동하여 각 25%의 확률
- 현재 에이젼트의 상태에 저장된 가치함수 = 0, 왼쪽 = 1, 밑 = 0.5, 위 = 0, 오른쪽 = 0
- 오른쪽으로 갈 경우 1의 보상
- 감가율 = 0.9
- 상태 변환 확률 = 1
- 가치 함수 업데이트 = 수식 2.33.

#### 수식 2.33 상태 변환 확률이 1인 벨만 기대 방정식

update

(1)각 행동을 취할 확률을 고려
(2) 각 행동을 했을 때 받을 보상,
(3) 다음 상태의 가치함수를 고려

#### 표 2.1. 벨만 기대 방정식의 계산

| 상태 | 행동   | value                      |
| ---- | ------ | -------------------------- |
| 1    | 상     | 0.25 * (0+0.9*0)=0         |
| 2    | 하     | 0.25*(0+0.9*0.5)=0.1125    |
| 3    | 좌     | 0.25*(0+0.9*1)=0.225       |
| 4    | 우     | 0.25*(1+0.9*0)=0.25        |
| 총합 | 기댓값 | 0+0.1125+0.225+0.25=0.5875 |

벨만 기대 방정식을 이용해 현재의 가치함수를 계속 업데이트하다보면 참값을 구할 수 있다. 하지만 여기서 참값은 최대로 받을 보상을 이야기하는 것이 아니다. 현재의 정책을 따라갔을 경우에 에이젼트가 얻을 실제 보상의 값에 대한 참 기댓값이다.

### 2. 벨만 최적 방정식

벨만 기대 방정식을 통해 계속 계산을 진행하다보면 식의 왼쪽 항과 오른쪽 항이 동일해진다.

#### 2.34 벨만 기대 방정식

처음에 가치함수의 값들은 (의미가 없는 값으로) 초기화된다. 초깃값으로부터 시작해서 수식 2.34의 벨만 기대 방정식으로 반복적으로 계산한다고 가정할 때, 이 계산을 반복하다 보면 방정식의 왼쪽 식과 오른쪽 식이 같아진다 (무한히 반복한다는 가정하에) 즉, v<sub>&pi;</sub>(s)값이 수렴한다. 그렇다면 현재 정책 &pi;</sub>에 대한 **참 가치함수**를 구한 것이다.

참 가치함수와 최적 가치함수 (Optimal Value Function)은 다르다.

가치함수 = 현재로부터 미래까지 받을 보상의 총합

- 참 가치함수 = '특정 정책'을 따라서 움직였을 경우에 받게 되는 보상에 대한 참값, 즉, 가치함수가 얼마가 될지에 대한 값

- 최적의 가치함수 = 수많은 정책 중에서 가장 높은 보상을 주는 가치함수

- 이 계산은 모든 상태에 대해 **동시에** 진행한다. 즉, 그리드월드의 경우 25개의 상태에 대해 동시에 계산한다.

- 단순히 현재 에이젼트의 정책에 대한 가치함수 뿐만 아니라 '최적 정책'을 찾고자 한다면?

- 가치함수를 통해 판단할 수 있음. 즉, 가치함수가 결국 정책이 얼마나 좋은지 말해줌.

#### 2.36 최적의 가치함수

v \*(s) = max<sub>&pi;</sub>[v<sub>&pi;</sub>(s)]

= 모든 정책에 대해 가장 큰 가치함수를 주는 정책(= 최적 정책)
= 모든 가능한 정책에 따른 v<sub>&pi;</sub>(s)값 중에서 최대를 반환하는 함수

---
