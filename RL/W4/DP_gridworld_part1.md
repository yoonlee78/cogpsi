# Reinforcement Learning

## Dynamic Programming with Gridworld : Part 1

### 2019. 7. 22. Mon

### CogPsi Study

### YK LEE

### 1. 순차적 행동 결정 문제

- 강화학습은 순차적으로 행동을 결정해야 하는 문제를 푸는 방법 중 하나
- 벨만 방정식을 통해 순차적 행동 결정 문제를 푸는 방법

[1. MDP정의] ==> [2. 벨만 방정식 계산] ==> [3. 최적 가치함수 + 최적 정책]

1. 순차적 행동 결정 문제를 MDP로 전환한다.
2. 가치함수를 벨만 기대 방정식으로 **반복적**으로 계산한다
3. 최적 가치함수와 최적 정책을 찾는다.

이번 장에서는 단계 2와 3에 대해 배울 것.
벨만 방정식 (복습) = 수식 3.1.을 만족하는 가치 함수를 찾는다.

#### 수식 3.1. 벨만 기대 방정식

---

### 2. 다이내믹 프로그래밍

벨만이 만든 최적화 방식 (강화 학습 이전에 존재)
큰 문제 안에 작은 문제가 중첩된 경우, 전체 큰 문제를 작은 문제로 쪼개서 푸는 방법

다이내믹 = 시간에 따라 변하는 것
프로그래밍 = (말 그대로) 계획하는 것 / 여러 프로세스가 다단계로 이뤄지는 것

하나의 프로세스를 대상으로 문제를 풀어나가는 것이 아닌, **시간에 따라 다른 프로세스들을 풀어나감**

#### 그림 3.2. 다이내믹 프로그래밍의 개념

#### 수식 3.2. 가치함수를 구하는 과정을 작은 과정으로 쪼개서 반복적으로 계산한다.

v<sub>0</sub>(S) -> v<sub1>1</sub>(s) -> ... -> v<sub>k</sub>(s) -> ... v<sub>&pi;</sub>(s)

- 이 때 수식 3.2의 한 번의 화살표는 한 번의 계산
- 그림 3.2의 iteration = k에서 iteration =k+1이 되는 과정
- 이 계산은 모든 상태에 대해 한 번 계산이 끝나면 모든 상태의 가치함수를 업데이트한다.
- 다음 계산은 업데이트된 가치 함수를 이용해 다시 똑같은 과정을 반복

### 2.2. 다이내믹 프로그래밍의 종류

- 정책 이터레이션 (policy iteration)
  벨만 기대 방정식을 이용해 순차적인 행동 결정 문제를 품
- 가치 이터레이션 (value iteration)
  벨만 최적 방정식을 이용해 문제를 품

### 3. GridWorld

#### 그림 3.3. 그리드월드 예제

빨간색 네모 : 에이젼트
파란색 원: 목적지 (보상 = +1)
연두색 세모 : 장애물 (보상 = -1)

문제: 에이젼트가 세모를 피해서 파란색으로 도착하여 +1의 보상을 받는 것. (단순히 +1을 받는 것이 아닌, 파란색에 도착하는 **최적 정책**을 찾는 것)

#### 3.1. 정책 이터레이션

강화학습 알고리즘의 흐름

[순차적 행동 결정문제] ==> [MDP] ==> 1. 벨만 기대 방정식 ==> 1.1. 정책 이터레이션 / 2. 벨만 최적 방정식 ==> 2.1. 가치 이터레이션 ==> \*살사 ==> \*\*큐러닝

\*, \*\* 이후 자세히 다룸

--
**정책 이터레이션**

- 다이내믹 프로그래밍의 한 종류, 벨만 기대 방정식을 사용해 MDP로 정의되는 문제를 품.

- 정책: 에이젼트가 모든 상태에서 어떻게 행동할지에 대한 정보

- MDP정의: 가장 높은 보상을 얻게 하는 정책을 찾는 것.

하지만 처음부터 이 정책을 알 수 없음. 따라서 처음에는 무작위 행동을 하는 정책(또는 특정한 정책)부터 시작하여 계속 발전시키는 방법을 사용.

#### 그림 3.5. 에이젼트의 처음 정책인 무작위 정책

정책 이터레이션에서는 "평가"를 "정책 평가(Policy Evaluation)", "발전"을 "정책 발전"(Policy Improvement)이라고 한다.

#### 그림 3.6 정책 평가와 정책 발전을 통해 새로운 정책으로 업데이트

### 정책 평가

#### 수식 3.3 정책에 대한 평가의 기준 = 가치 함수

하지만 더 먼 미래까지 고려할수록 일어날 수 있는 경우의 수가 기하급수적으로 늘어나기때문에 다이내믹 프로그래밍 방법을 사용함.

주변 상태의 가치함수와 한 타임스텝의 보상만 고려해서 현재 상태의 다음 가치함수를 계산
= 한 타임스텝의 보상만 고려하고 주변 상태의 가치함수들은 참 가치함수가 아님. (= 이런 계산을 여러번 반복함으로써 참 값으로 수렴함)

#### 수식 3.4. 벨만 기대 방정식을 통한 효율적인 가치함수 계산

==>

#### 수식 3.5. 합의 형태로 표현한 벨만 기대 방정식

정책 평가는 &pi; 라는 정책에 대해 반복적으로 수행하는 것이므로 계산의 단계를 표현할 새로운 변수 k 1,2,3,4...를 설정한다.

#### 수식 3.6. k번째 가치함수를 통해 k+1번째 가치함수를 계산하는 방정식

#### 그림 3.7 그리드월드에서 가치함수의 계산

(간단한 벨만 기대 방정식을 이용해 가치함수를 업데이트하는 예제)

정책 = 무작위 정책, 상/하/좌/우를 선택할 확률은 각각 0.25. A = 에이젼트, 그 위 = 현재 에이젼트의 상태. 상태 변환 확률 = 모든 상황에 대해 1이라고 가정, 감가율 =0.9

상: 0.25 x (0 + 0.9 x 0 ) = 0 <br>
하: 0.25 x (0 + 0.9 x 0.5) = 0.1125 <br>
좌: 0.25 x (0 + 0.9 x 1) = 0.225 <br>
우: 0.25 x (1 + 0.9 x 0) = 0.25 <br>

다음 가치 함수 = 0 + 0.1125 + 0.225 + 0.25

#### 그림 3.8 벨만 기대 방정식을 이용한 현재 상태의 가치함수 업데이트

한 번의 정책 평가 과정을 순서대로 나타내면 아래와 같다.

1. k번째 가치함수 행렬에서 현재 상태 s에서 갈 수 있는 다음 상태 s'에 저장돼 있는 가치 함수 v<sub>k</sub>(s')을 불러온다 (보라색 부분 중 하나)

2. v<sub>k</sub>(s')에 감가율 r을 곱하고 그 상태로 가는 행동에 대한 보상 R<sup>a</sup><sub>s</sub>을 더한다.

3. 2번에서 구한 값에 그 행동을 할 확률인 **정책**을 곱한다.

&pi;(a|s)(R<sup>a</sup><sub>s</sub> + &gamma;v<sub>k</sub>(s'))

4. 3번을 모든 선택 가능한 행동에 대해 반복하고 그 값들을 더한다.

&sum;<sub>a &isin; A </sub> &pi; (a|s)(R<sup>a</sup><sub>s</sub> + &gamma;v<sub>k</sub>(s'))

5. 4번 과정을 통해 더한 값을 k+1번째 가치함수 행렬에 상태 s자리에 저장한다

6. 1부터 5 과정을 모든 s &isin; S 대해 반복한다.

v<sub>1</sub>로 시작해서 무한히 이러한 정책 평가를 반복하면 참 v<sub>&pi;</sub>가 된다.

### 정책 발전

정책 평가를 바탕으로 정책을 발전시킨다. 정책 발전의 방법이 정해져 있는 것은 아니다. 우선 가장 널리 알려진 **탐욕 정책 발전**<sup>Greedy Policy Improvement</sup>을 소개한다.

예시. 큐함수를 이용한 탐욕 정책

#### 수식 3.8. Q 함수

#### 수식 3.9. 계산 가능한 Q함수

#### 수식 3.10. 탐욕 정책 발전으로 얻은 새로운 정책

&pi; (s) = argmzx<sub>a&isin;A</sub>q<sub>&pi;</sub>(s,a)

argmax = 가장 큰 큐함수를 가지는 행동을 반환하는 함수 (max함수와는 다르게 반환되는 것은 '행동'이다)

#### 그림 3.9. 큐함수의 값이 제일 높은 행동을 선택하는 탐욕 정책 발전

큐함수를 통해(argmax)얻은 정책은 특정 상태(보라색 상태)로 가는 **행동**만 선택한다.
