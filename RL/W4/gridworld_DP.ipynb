{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "## Dynamic Programming with Gridworld : Part 1\n",
    "\n",
    "### 2019. 7. 22. Mon\n",
    "\n",
    "### CogPsi Study\n",
    "\n",
    "### YK LEE\n",
    "\n",
    "### 1. 순차적 행동 결정 문제\n",
    "\n",
    "- 강화학습은 순차적으로 행동을 결정해야 하는 문제를 푸는 방법 중 하나\n",
    "- 벨만 방정식을 통해 순차적 행동 결정 문제를 푸는 방법\n",
    "\n",
    "[1. MDP정의] ==> [2. 벨만 방정식 계산] ==> [3. 최적 가치함수 + 최적 정책]\n",
    "\n",
    "1. 순차적 행동 결정 문제를 MDP로 전환한다.\n",
    "2. 가치함수를 벨만 기대 방정식으로 **반복적**으로 계산한다\n",
    "3. 최적 가치함수와 최적 정책을 찾는다.\n",
    "\n",
    "이번 장에서는 단계 2와 3에 대해 배울 것.\n",
    "벨만 방정식 (복습) = 수식 3.1.을 만족하는 가치 함수를 찾는다.\n",
    "\n",
    "#### 수식 3.1. 벨만 기대 방정식\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 다이내믹 프로그래밍\n",
    "\n",
    "벨만이 만든 최적화 방식 (강화 학습 이전에 존재)\n",
    "큰 문제 안에 작은 문제가 중첩된 경우, 전체 큰 문제를 작은 문제로 쪼개서 푸는 방법\n",
    "\n",
    "다이내믹 = 시간에 따라 변하는 것\n",
    "프로그래밍 = (말 그대로) 계획하는 것 / 여러 프로세스가 다단계로 이뤄지는 것\n",
    "\n",
    "하나의 프로세스를 대상으로 문제를 풀어나가는 것이 아닌, **시간에 따라 다른 프로세스들을 풀어나감**\n",
    "\n",
    "#### 그림 3.2. 다이내믹 프로그래밍의 개념\n",
    "\n",
    "#### 수식 3.2. 가치함수를 구하는 과정을 작은 과정으로 쪼개서 반복적으로 계산한다.\n",
    "\n",
    "v<sub>0</sub>(S) -> v<sub1>1</sub>(s) -> ... -> v<sub>k</sub>(s) -> ... v<sub>&pi;</sub>(s)\n",
    "\n",
    "- 이 때 수식 3.2의 한 번의 화살표는 한 번의 계산\n",
    "- 그림 3.2의 iteration = k에서 iteration =k+1이 되는 과정\n",
    "- 이 계산은 모든 상태에 대해 한 번 계산이 끝나면 모든 상태의 가치함수를 업데이트한다.\n",
    "- 다음 계산은 업데이트된 가치 함수를 이용해 다시 똑같은 과정을 반복\n",
    "\n",
    "### 2.2. 다이내믹 프로그래밍의 종류\n",
    "\n",
    "- 정책 이터레이션 (policy iteration)\n",
    "  벨만 기대 방정식을 이용해 순차적인 행동 결정 문제를 품\n",
    "- 가치 이터레이션 (value iteration)\n",
    "  벨만 최적 방정식을 이용해 문제를 품\n",
    "\n",
    "### 3. GridWorld\n",
    "\n",
    "#### 그림 3.3. 그리드월드 예제\n",
    "\n",
    "빨간색 네모 : 에이젼트\n",
    "파란색 원: 목적지 (보상 = +1)\n",
    "연두색 세모 : 장애물 (보상 = -1)\n",
    "\n",
    "문제: 에이젼트가 세모를 피해서 파란색으로 도착하여 +1의 보상을 받는 것. (단순히 +1을 받는 것이 아닌, 파란색에 도착하는 **최적 정책**을 찾는 것)\n",
    "\n",
    "#### 3.1. 정책 이터레이션\n",
    "\n",
    "강화학습 알고리즘의 흐름\n",
    "\n",
    "[순차적 행동 결정문제] ==> [MDP] ==> 1. 벨만 기대 방정식 ==> 1.1. 정책 이터레이션 / 2. 벨만 최적 방정식 ==> 2.1. 가치 이터레이션 ==> \\*살사 ==> \\*\\*큐러닝\n",
    "\n",
    "\\*, \\*\\* 이후 자세히 다룸\n",
    "\n",
    "--\n",
    "**정책 이터레이션**\n",
    "\n",
    "- 다이내믹 프로그래밍의 한 종류, 벨만 기대 방정식을 사용해 MDP로 정의되는 문제를 품.\n",
    "\n",
    "- 정책: 에이젼트가 모든 상태에서 어떻게 행동할지에 대한 정보\n",
    "\n",
    "- MDP정의: 가장 높은 보상을 얻게 하는 정책을 찾는 것.\n",
    "\n",
    "하지만 처음부터 이 정책을 알 수 없음. 따라서 처음에는 무작위 행동을 하는 정책(또는 특정한 정책)부터 시작하여 계속 발전시키는 방법을 사용.\n",
    "\n",
    "#### 그림 3.5. 에이젼트의 처음 정책인 무작위 정책\n",
    "\n",
    "정책 이터레이션에서는 \"평가\"를 \"정책 평가(Policy Evaluation)\", \"발전\"을 \"정책 발전\"(Policy Improvement)이라고 한다.\n",
    "\n",
    "#### 그림 3.6 정책 평가와 정책 발전을 통해 새로운 정책으로 업데이트\n",
    "\n",
    "### 정책 평가\n",
    "\n",
    "#### 수식 3.3 정책에 대한 평가의 기준 = 가치 함수\n",
    "\n",
    "하지만 더 먼 미래까지 고려할수록 일어날 수 있는 경우의 수가 기하급수적으로 늘어나기때문에 다이내믹 프로그래밍 방법을 사용함.\n",
    "\n",
    "주변 상태의 가치함수와 한 타임스텝의 보상만 고려해서 현재 상태의 다음 가치함수를 계산\n",
    "= 한 타임스텝의 보상만 고려하고 주변 상태의 가치함수들은 참 가치함수가 아님. (= 이런 계산을 여러번 반복함으로써 참 값으로 수렴함)\n",
    "\n",
    "#### 수식 3.4. 벨만 기대 방정식을 통한 효율적인 가치함수 계산\n",
    "\n",
    "==>\n",
    "\n",
    "#### 수식 3.5. 합의 형태로 표현한 벨만 기대 방정식\n",
    "\n",
    "정책 평가는 &pi; 라는 정책에 대해 반복적으로 수행하는 것이므로 계산의 단계를 표현할 새로운 변수 k 1,2,3,4...를 설정한다.\n",
    "\n",
    "#### 수식 3.6. k번째 가치함수를 통해 k+1번째 가치함수를 계산하는 방정식\n",
    "\n",
    "#### 그림 3.7 그리드월드에서 가치함수의 계산\n",
    "\n",
    "(간단한 벨만 기대 방정식을 이용해 가치함수를 업데이트하는 예제)\n",
    "\n",
    "정책 = 무작위 정책, 상/하/좌/우를 선택할 확률은 각각 0.25. A = 에이젼트, 그 위 = 현재 에이젼트의 상태. 상태 변환 확률 = 모든 상황에 대해 1이라고 가정, 감가율 =0.9\n",
    "\n",
    "상: 0.25 x (0 + 0.9 x 0 ) = 0 <br>\n",
    "하: 0.25 x (0 + 0.9 x 0.5) = 0.1125 <br>\n",
    "좌: 0.25 x (0 + 0.9 x 1) = 0.225 <br>\n",
    "우: 0.25 x (1 + 0.9 x 0) = 0.25 <br>\n",
    "\n",
    "다음 가치 함수 = 0 + 0.1125 + 0.225 + 0.25\n",
    "\n",
    "#### 그림 3.8 벨만 기대 방정식을 이용한 현재 상태의 가치함수 업데이트\n",
    "\n",
    "한 번의 정책 평가 과정을 순서대로 나타내면 아래와 같다.\n",
    "\n",
    "1. k번째 가치함수 행렬에서 현재 상태 s에서 갈 수 있는 다음 상태 s'에 저장돼 있는 가치 함수 v<sub>k</sub>(s')을 불러온다 (보라색 부분 중 하나)\n",
    "\n",
    "2. v<sub>k</sub>(s')에 감가율 r을 곱하고 그 상태로 가는 행동에 대한 보상 R<sup>a</sup><sub>s</sub>을 더한다.\n",
    "\n",
    "3. 2번에서 구한 값에 그 행동을 할 확률인 **정책**을 곱한다.\n",
    "\n",
    "&pi;(a|s)(R<sup>a</sup><sub>s</sub> + &gamma;v<sub>k</sub>(s'))\n",
    "\n",
    "4. 3번을 모든 선택 가능한 행동에 대해 반복하고 그 값들을 더한다.\n",
    "\n",
    "&sum;<sub>a &isin; A </sub> &pi; (a|s)(R<sup>a</sup><sub>s</sub> + &gamma;v<sub>k</sub>(s'))\n",
    "\n",
    "5. 4번 과정을 통해 더한 값을 k+1번째 가치함수 행렬에 상태 s자리에 저장한다\n",
    "\n",
    "6. 1부터 5 과정을 모든 s &isin; S 대해 반복한다.\n",
    "\n",
    "v<sub>1</sub>로 시작해서 무한히 이러한 정책 평가를 반복하면 참 v<sub>&pi;</sub>가 된다.\n",
    "\n",
    "### 정책 발전\n",
    "\n",
    "정책 평가를 바탕으로 정책을 발전시킨다. 정책 발전의 방법이 정해져 있는 것은 아니다. 우선 가장 널리 알려진 **탐욕 정책 발전**<sup>Greedy Policy Improvement</sup>을 소개한다.\n",
    "\n",
    "예시. 큐함수를 이용한 탐욕 정책\n",
    "\n",
    "#### 수식 3.8. Q 함수\n",
    "\n",
    "#### 수식 3.9. 계산 가능한 Q함수\n",
    "\n",
    "#### 수식 3.10. 탐욕 정책 발전으로 얻은 새로운 정책\n",
    "\n",
    "&pi; (s) = argmzx<sub>a&isin;A</sub>q<sub>&pi;</sub>(s,a)\n",
    "\n",
    "argmax = 가장 큰 큐함수를 가지는 행동을 반환하는 함수 (max함수와는 다르게 반환되는 것은 '행동'이다)\n",
    "\n",
    "#### 그림 3.9. 큐함수의 값이 제일 높은 행동을 선택하는 탐욕 정책 발전\n",
    "\n",
    "큐함수를 통해(argmax)얻은 정책은 특정 상태(보라색 상태)로 가는 **행동**만 선택한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 알고리즘 (실습)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RLCode github repository: https://github.com/rlcode/reinforcement-learning/tree/master/1-grid-world/1-policy-iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. policy_iteration.py \n",
    "PolicyIteration클래스를 포함, 정책이터레이션의 알고리즘 관련 함수와 main 함수가 정의되어있음\n",
    "\n",
    "2. environment.py\n",
    "그리드월드 예제의 화면을 구성하고 상태, 보상 등을 포함한 환경에 대한 정보를 제공하기 위한 함수로 구성되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import Button\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import ImageTk, Image\n",
    "\n",
    "PhotoImage = ImageTk.PhotoImage\n",
    "UNIT = 100  # pixels\n",
    "HEIGHT = 5  # grid height\n",
    "WIDTH = 5  # grid width\n",
    "TRANSITION_PROB = 1\n",
    "POSSIBLE_ACTIONS = [0, 1, 2, 3]  # up, down, left, right\n",
    "ACTIONS = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # actions in coordinates\n",
    "REWARDS = []\n",
    "\n",
    "\n",
    "class GraphicDisplay(tk.Tk):\n",
    "    def __init__(self, agent):\n",
    "        super(GraphicDisplay, self).__init__()\n",
    "        self.title('Policy Iteration')\n",
    "        self.geometry('{0}x{1}'.format(HEIGHT * UNIT, HEIGHT * UNIT + 50))\n",
    "        self.texts = []\n",
    "        self.arrows = []\n",
    "        self.env = Env()\n",
    "        self.agent = agent\n",
    "        self.evaluation_count = 0\n",
    "        self.improvement_count = 0\n",
    "        self.is_moving = 0\n",
    "        (self.up, self.down, self.left, self.right), self.shapes = self.load_images()\n",
    "        self.canvas = self._build_canvas()\n",
    "        self.text_reward(2, 2, \"R : 1.0\")\n",
    "        self.text_reward(1, 2, \"R : -1.0\")\n",
    "        self.text_reward(2, 1, \"R : -1.0\")\n",
    "\n",
    "    def _build_canvas(self):\n",
    "        canvas = tk.Canvas(self, bg='white',\n",
    "                           height=HEIGHT * UNIT,\n",
    "                           width=WIDTH * UNIT)\n",
    "        # buttons\n",
    "        iteration_button = Button(self, text=\"Evaluate\",\n",
    "                                  command=self.evaluate_policy)\n",
    "        iteration_button.configure(width=10, activebackground=\"#33B5E5\")\n",
    "        canvas.create_window(WIDTH * UNIT * 0.13, HEIGHT * UNIT + 10,\n",
    "                             window=iteration_button)\n",
    "        policy_button = Button(self, text=\"Improve\",\n",
    "                               command=self.improve_policy)\n",
    "        policy_button.configure(width=10, activebackground=\"#33B5E5\")\n",
    "        canvas.create_window(WIDTH * UNIT * 0.37, HEIGHT * UNIT + 10,\n",
    "                             window=policy_button)\n",
    "        policy_button = Button(self, text=\"move\", command=self.move_by_policy)\n",
    "        policy_button.configure(width=10, activebackground=\"#33B5E5\")\n",
    "        canvas.create_window(WIDTH * UNIT * 0.62, HEIGHT * UNIT + 10,\n",
    "                             window=policy_button)\n",
    "        policy_button = Button(self, text=\"reset\", command=self.reset)\n",
    "        policy_button.configure(width=10, activebackground=\"#33B5E5\")\n",
    "        canvas.create_window(WIDTH * UNIT * 0.87, HEIGHT * UNIT + 10,\n",
    "                             window=policy_button)\n",
    "\n",
    "        # create grids\n",
    "        for col in range(0, WIDTH * UNIT, UNIT):  # 0~400 by 80\n",
    "            x0, y0, x1, y1 = col, 0, col, HEIGHT * UNIT\n",
    "            canvas.create_line(x0, y0, x1, y1)\n",
    "        for row in range(0, HEIGHT * UNIT, UNIT):  # 0~400 by 80\n",
    "            x0, y0, x1, y1 = 0, row, HEIGHT * UNIT, row\n",
    "            canvas.create_line(x0, y0, x1, y1)\n",
    "\n",
    "        # add img to canvas\n",
    "        self.rectangle = canvas.create_image(50, 50, image=self.shapes[0])\n",
    "        canvas.create_image(250, 150, image=self.shapes[1])\n",
    "        canvas.create_image(150, 250, image=self.shapes[1])\n",
    "        canvas.create_image(250, 250, image=self.shapes[2])\n",
    "\n",
    "        # pack all\n",
    "        canvas.pack()\n",
    "\n",
    "        return canvas\n",
    "\n",
    "    def load_images(self):\n",
    "        up = PhotoImage(Image.open(\"../img/up.png\").resize((13, 13)))\n",
    "        right = PhotoImage(Image.open(\"../img/right.png\").resize((13, 13)))\n",
    "        left = PhotoImage(Image.open(\"../img/left.png\").resize((13, 13)))\n",
    "        down = PhotoImage(Image.open(\"../img/down.png\").resize((13, 13)))\n",
    "        rectangle = PhotoImage(Image.open(\"../img/rectangle.png\").resize((65, 65)))\n",
    "        triangle = PhotoImage(Image.open(\"../img/triangle.png\").resize((65, 65)))\n",
    "        circle = PhotoImage(Image.open(\"../img/circle.png\").resize((65, 65)))\n",
    "        return (up, down, left, right), (rectangle, triangle, circle)\n",
    "\n",
    "    def reset(self):\n",
    "        if self.is_moving == 0:\n",
    "            self.evaluation_count = 0\n",
    "            self.improvement_count = 0\n",
    "            for i in self.texts:\n",
    "                self.canvas.delete(i)\n",
    "\n",
    "            for i in self.arrows:\n",
    "                self.canvas.delete(i)\n",
    "            self.agent.value_table = [[0.0] * WIDTH for _ in range(HEIGHT)]\n",
    "            self.agent.policy_table = ([[[0.25, 0.25, 0.25, 0.25]] * WIDTH\n",
    "                                        for _ in range(HEIGHT)])\n",
    "            self.agent.policy_table[2][2] = []\n",
    "            x, y = self.canvas.coords(self.rectangle)\n",
    "            self.canvas.move(self.rectangle, UNIT / 2 - x, UNIT / 2 - y)\n",
    "\n",
    "    def text_value(self, row, col, contents, font='Helvetica', size=10,\n",
    "                   style='normal', anchor=\"nw\"):\n",
    "        origin_x, origin_y = 85, 70\n",
    "        x, y = origin_y + (UNIT * col), origin_x + (UNIT * row)\n",
    "        font = (font, str(size), style)\n",
    "        text = self.canvas.create_text(x, y, fill=\"black\", text=contents,\n",
    "                                       font=font, anchor=anchor)\n",
    "        return self.texts.append(text)\n",
    "\n",
    "    def text_reward(self, row, col, contents, font='Helvetica', size=10,\n",
    "                    style='normal', anchor=\"nw\"):\n",
    "        origin_x, origin_y = 5, 5\n",
    "        x, y = origin_y + (UNIT * col), origin_x + (UNIT * row)\n",
    "        font = (font, str(size), style)\n",
    "        text = self.canvas.create_text(x, y, fill=\"black\", text=contents,\n",
    "                                       font=font, anchor=anchor)\n",
    "        return self.texts.append(text)\n",
    "\n",
    "    def rectangle_move(self, action):\n",
    "        base_action = np.array([0, 0])\n",
    "        location = self.find_rectangle()\n",
    "        self.render()\n",
    "        if action == 0 and location[0] > 0:  # up\n",
    "            base_action[1] -= UNIT\n",
    "        elif action == 1 and location[0] < HEIGHT - 1:  # down\n",
    "            base_action[1] += UNIT\n",
    "        elif action == 2 and location[1] > 0:  # left\n",
    "            base_action[0] -= UNIT\n",
    "        elif action == 3 and location[1] < WIDTH - 1:  # right\n",
    "            base_action[0] += UNIT\n",
    "        # move agent\n",
    "        self.canvas.move(self.rectangle, base_action[0], base_action[1])\n",
    "\n",
    "    def find_rectangle(self):\n",
    "        temp = self.canvas.coords(self.rectangle)\n",
    "        x = (temp[0] / 100) - 0.5\n",
    "        y = (temp[1] / 100) - 0.5\n",
    "        return int(y), int(x)\n",
    "\n",
    "    def move_by_policy(self):\n",
    "        if self.improvement_count != 0 and self.is_moving != 1:\n",
    "            self.is_moving = 1\n",
    "\n",
    "            x, y = self.canvas.coords(self.rectangle)\n",
    "            self.canvas.move(self.rectangle, UNIT / 2 - x, UNIT / 2 - y)\n",
    "\n",
    "            x, y = self.find_rectangle()\n",
    "            while len(self.agent.policy_table[x][y]) != 0:\n",
    "                self.after(100,\n",
    "                           self.rectangle_move(self.agent.get_action([x, y])))\n",
    "                x, y = self.find_rectangle()\n",
    "            self.is_moving = 0\n",
    "\n",
    "    def draw_one_arrow(self, col, row, policy):\n",
    "        if col == 2 and row == 2:\n",
    "            return\n",
    "\n",
    "        if policy[0] > 0:  # up\n",
    "            origin_x, origin_y = 50 + (UNIT * row), 10 + (UNIT * col)\n",
    "            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n",
    "                                                        image=self.up))\n",
    "        if policy[1] > 0:  # down\n",
    "            origin_x, origin_y = 50 + (UNIT * row), 90 + (UNIT * col)\n",
    "            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n",
    "                                                        image=self.down))\n",
    "        if policy[2] > 0:  # left\n",
    "            origin_x, origin_y = 10 + (UNIT * row), 50 + (UNIT * col)\n",
    "            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n",
    "                                                        image=self.left))\n",
    "        if policy[3] > 0:  # right\n",
    "            origin_x, origin_y = 90 + (UNIT * row), 50 + (UNIT * col)\n",
    "            self.arrows.append(self.canvas.create_image(origin_x, origin_y,\n",
    "                                                        image=self.right))\n",
    "\n",
    "    def draw_from_policy(self, policy_table):\n",
    "        for i in range(HEIGHT):\n",
    "            for j in range(WIDTH):\n",
    "                self.draw_one_arrow(i, j, policy_table[i][j])\n",
    "\n",
    "    def print_value_table(self, value_table):\n",
    "        for i in range(WIDTH):\n",
    "            for j in range(HEIGHT):\n",
    "                self.text_value(i, j, value_table[i][j])\n",
    "\n",
    "    def render(self):\n",
    "        time.sleep(0.1)\n",
    "        self.canvas.tag_raise(self.rectangle)\n",
    "        self.update()\n",
    "\n",
    "    def evaluate_policy(self):\n",
    "        self.evaluation_count += 1\n",
    "        for i in self.texts:\n",
    "            self.canvas.delete(i)\n",
    "        self.agent.policy_evaluation()\n",
    "        self.print_value_table(self.agent.value_table)\n",
    "\n",
    "    def improve_policy(self):\n",
    "        self.improvement_count += 1\n",
    "        for i in self.arrows:\n",
    "            self.canvas.delete(i)\n",
    "        self.agent.policy_improvement()\n",
    "        self.draw_from_policy(self.agent.policy_table)\n",
    "\n",
    "\n",
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.transition_probability = TRANSITION_PROB\n",
    "        self.width = WIDTH\n",
    "        self.height = HEIGHT\n",
    "        self.reward = [[0] * WIDTH for _ in range(HEIGHT)]\n",
    "        self.possible_actions = POSSIBLE_ACTIONS\n",
    "        self.reward[2][2] = 1  # reward 1 for circle\n",
    "        self.reward[1][2] = -1  # reward -1 for triangle\n",
    "        self.reward[2][1] = -1  # reward -1 for triangle\n",
    "        self.all_state = []\n",
    "\n",
    "        for x in range(WIDTH):\n",
    "            for y in range(HEIGHT):\n",
    "                state = [x, y]\n",
    "                self.all_state.append(state)\n",
    "\n",
    "    def get_reward(self, state, action):\n",
    "        next_state = self.state_after_action(state, action)\n",
    "        return self.reward[next_state[0]][next_state[1]]\n",
    "\n",
    "    def state_after_action(self, state, action_index):\n",
    "        action = ACTIONS[action_index]\n",
    "        return self.check_boundary([state[0] + action[0], state[1] + action[1]])\n",
    "\n",
    "    @staticmethod\n",
    "    def check_boundary(state):\n",
    "        state[0] = (0 if state[0] < 0 else WIDTH - 1\n",
    "                    if state[0] > WIDTH - 1 else state[0])\n",
    "        state[1] = (0 if state[1] < 0 else HEIGHT - 1\n",
    "                    if state[1] > HEIGHT - 1 else state[1])\n",
    "        return state\n",
    "\n",
    "    def get_transition_prob(self, state, action):\n",
    "        return self.transition_probability\n",
    "\n",
    "    def get_all_states(self):\n",
    "        return self.all_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DP에서 에이젼트는 환경의 모든 정보를 알고있다. 이 정보를 통해 에이젼트는 최적 정책을 찾는 계산을 한다. 계산에 필요한 정보와 함수는 environment.py 안에 Env 클래스로 정의되어 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.transition_probability = TRANSITION_PROB\n",
    "        self.width = WIDTH #그리드의 너비 (그리드월드의 가로, 세로를 정수로 반환)\n",
    "        self.height = HEIGHT #그리드의 높이 (그리드월드의 가로, 세로를 정수로 반환)\n",
    "        self.reward = [[0] * WIDTH for _ in range(HEIGHT)]\n",
    "        self.possible_actions = POSSIBLE_ACTIONS #상,하,좌,우 #[0,1,2,3]을 반환, 순서대로 상,하, 좌, 우를 의미\n",
    "        self.reward[2][2] = 1  # reward 1 for circle\n",
    "        self.reward[1][2] = -1  # reward -1 for triangle\n",
    "        self.reward[2][1] = -1  # reward -1 for triangle\n",
    "        self.all_state = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def state_after_action(self, state, action_index): #특정 상태에서 특정 행동을 했을 때 에이젼트가 가는 다음 상태 (행동 후의 상태를 좌표로 표현한 리스트를 변환 (예 [1,2]))\n",
    "        action = ACTIONS[action_index]\n",
    "        return self.check_boundary([state[0] + action[0], state[1] + action[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_all_states(self):\n",
    "        return self.all_state\n",
    "#존재하는 모든 상태 \n",
    "# 모든 상태를 반환 (예: [0,0],[0,1]...[4,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_reward(self, state, action):\n",
    "        next_state = self.state_after_action(state, action)\n",
    "        return self.reward[next_state[0]][next_state[1]]\n",
    "#.get_reward(state)특정 상태의 보상\n",
    "#정수의 형태로 보상을 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "from environment import GraphicDisplay, Env\n",
    "\n",
    "\n",
    "class PolicyIteration:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        # 2-d list for the value function\n",
    "        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n",
    "        # list of random policy (same probability of up, down, left, right)\n",
    "        self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width\n",
    "                                    for _ in range(env.height)]\n",
    "        # setting terminal state\n",
    "        self.policy_table[2][2] = []\n",
    "        self.discount_factor = 0.9\n",
    "\n",
    "    def policy_evaluation(self):\n",
    "        next_value_table = [[0.00] * self.env.width\n",
    "                                    for _ in range(self.env.height)]\n",
    "\n",
    "        # Bellman Expectation Equation for the every states\n",
    "        for state in self.env.get_all_states():\n",
    "            value = 0.0\n",
    "            # keep the value function of terminal states as 0\n",
    "            if state == [2, 2]:\n",
    "                next_value_table[state[0]][state[1]] = value\n",
    "                continue\n",
    "\n",
    "            for action in self.env.possible_actions:\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                value += (self.get_policy(state)[action] *\n",
    "                          (reward + self.discount_factor * next_value))\n",
    "\n",
    "            next_value_table[state[0]][state[1]] = round(value, 2)\n",
    "\n",
    "        self.value_table = next_value_table\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        next_policy = self.policy_table\n",
    "        for state in self.env.get_all_states():\n",
    "            if state == [2, 2]:\n",
    "                continue\n",
    "            value = -99999\n",
    "            max_index = []\n",
    "            result = [0.0, 0.0, 0.0, 0.0]  # initialize the policy\n",
    "\n",
    "            # for every actions, calculate\n",
    "            # [reward + (discount factor) * (next state value function)]\n",
    "            for index, action in enumerate(self.env.possible_actions):\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                temp = reward + self.discount_factor * next_value\n",
    "\n",
    "                # We normally can't pick multiple actions in greedy policy.\n",
    "                # but here we allow multiple actions with same max values\n",
    "                if temp == value:\n",
    "                    max_index.append(index)\n",
    "                elif temp > value:\n",
    "                    value = temp\n",
    "                    max_index.clear()\n",
    "                    max_index.append(index)\n",
    "\n",
    "            # probability of action\n",
    "            prob = 1 / len(max_index)\n",
    "\n",
    "            for index in max_index:\n",
    "                result[index] = prob\n",
    "\n",
    "            next_policy[state[0]][state[1]] = result\n",
    "\n",
    "        self.policy_table = next_policy\n",
    "\n",
    "    # get action according to the current policy\n",
    "    def get_action(self, state):\n",
    "        random_pick = random.randrange(100) / 100\n",
    "\n",
    "        policy = self.get_policy(state)\n",
    "        policy_sum = 0.0\n",
    "        # return the action in the index\n",
    "        for index, value in enumerate(policy):\n",
    "            policy_sum += value\n",
    "            if random_pick < policy_sum:\n",
    "                return index\n",
    "\n",
    "    # get policy of specific state\n",
    "    def get_policy(self, state):\n",
    "        if state == [2, 2]:\n",
    "            return 0.0\n",
    "        return self.policy_table[state[0]][state[1]]\n",
    "\n",
    "    def get_value(self, state):\n",
    "        return round(self.value_table[state[0]][state[1]], 2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    policy_iteration = PolicyIteration(env)\n",
    "    grid_world = GraphicDisplay(policy_iteration)\n",
    "    grid_world.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation = policy_evaluation ()\n",
    "#### Improvement = policy_improvement()\n",
    "#### Move = get_action(state)\n",
    "#### Clear = 모든 변수 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    def __init__(self, env):\n",
    "        ####환경에 대한 객체\n",
    "        self.env = env\n",
    "        # 2-d list for the value function\n",
    "        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n",
    "        # list of random policy (same probability of up, down, left, right)\n",
    "        self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width\n",
    "                                    for _ in range(env.height)]\n",
    "        # setting terminal state\n",
    "        self.policy_table[2][2] = []\n",
    "        self.discount_factor = 0.9\n",
    "\n",
    "####정책 평가\n",
    "    def policy_evaluation(self):\n",
    "        next_value_table = [[0.00] * self.env.width\n",
    "                                    for _ in range(self.env.height)]\n",
    "\n",
    "        # Bellman Expectation Equation for the every states\n",
    "        for state in self.env.get_all_states():\n",
    "            value = 0.0\n",
    "            # keep the value function of terminal states as 0\n",
    "            if state == [2, 2]:\n",
    "                next_value_table[state[0]][state[1]] = value\n",
    "                continue\n",
    "\n",
    "            for action in self.env.possible_actions:\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                value += (self.get_policy(state)[action] *\n",
    "                          (reward + self.discount_factor * next_value))\n",
    "\n",
    "            next_value_table[state[0]][state[1]] = round(value, 2)\n",
    "\n",
    "        self.value_table = next_value_table\n",
    "\n",
    "####정책 발전\n",
    "    def policy_improvement(self):\n",
    "        next_policy = self.policy_table\n",
    "        for state in self.env.get_all_states():\n",
    "            if state == [2, 2]:\n",
    "                continue\n",
    "            value = -99999\n",
    "            max_index = []\n",
    "            result = [0.0, 0.0, 0.0, 0.0]  # initialize the policy\n",
    "\n",
    "            # for every actions, calculate\n",
    "            # [reward + (discount factor) * (next state value function)]\n",
    "            for index, action in enumerate(self.env.possible_actions):\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                temp = reward + self.discount_factor * next_value\n",
    "\n",
    "                # We normally can't pick multiple actions in greedy policy.\n",
    "                # but here we allow multiple actions with same max values\n",
    "                if temp == value:\n",
    "                    max_index.append(index)\n",
    "                elif temp > value:\n",
    "                    value = temp\n",
    "                    max_index.clear()\n",
    "                    max_index.append(index)\n",
    "\n",
    "            # probability of action\n",
    "            prob = 1 / len(max_index)\n",
    "\n",
    "            for index in max_index:\n",
    "                result[index] = prob\n",
    "\n",
    "            next_policy[state[0]][state[1]] = result\n",
    "\n",
    "        self.policy_table = next_policy\n",
    "\n",
    "    #### 특정 상태에서 정책에 따른 행동\n",
    "    def get_action(self, state):\n",
    "        random_pick = random.randrange(100) / 100\n",
    "\n",
    "        policy = self.get_policy(state)\n",
    "        policy_sum = 0.0\n",
    "        # return the action in the index\n",
    "        for index, value in enumerate(policy):\n",
    "            policy_sum += value\n",
    "            if random_pick < policy_sum:\n",
    "                return index\n",
    "\n",
    "    # get policy of specific state\n",
    "    def get_policy(self, state):\n",
    "        if state == [2, 2]:\n",
    "            return 0.0\n",
    "        return self.policy_table[state[0]][state[1]]\n",
    "\n",
    "    def get_value(self, state):\n",
    "        return round(self.value_table[state[0]][state[1]], 2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = Env()\n",
    "    policy_iteration = PolicyIteration(env)\n",
    "    grid_world = GraphicDisplay(policy_iteration)\n",
    "    grid_world.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정책 평가 \n",
    "def policy_evaluation(self):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정책 평가 세부 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    def __init__(self, env):\n",
    "        ####환경에 대한 객체\n",
    "        self.env = env\n",
    "        # value_table 가치 함수를 2차원 리스트로 초기화 (세로 5칸, 가로 5칸의 크기이므로 5x5의 2차원 리스트가 된다)\n",
    "        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n",
    "        # policy table 상,하,좌,우에 해당하는 각 행동의 확률을 담고있는 리스트 (5*5*4)의 3차원 리스트, 무작위 정책으로 초기화\n",
    "        self.policy_table = [[[0.25, 0.25, 0.25, 0.25]] * env.width\n",
    "                                    for _ in range(env.height)]\n",
    "        # setting terminal state\n",
    "        self.policy_table[2][2] = []\n",
    "        self.discount_factor = 0.9 #벨만 방정식에 사용되는 감가율 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. policy_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정책 평가에서 에이젼트는 모든 상태의 가치함수를 업데이트한다. 모든 상태의 가치함수를 업데이트하기위해 next_value_table을 선언한 다음 계산 결과를 next_value_table에 저장한다. \n",
    "\n",
    "그리고 모든 상태에 대해 벨만 기대 방정식의 계싼이 끝나면 현재의 value_table에 next_value_table을 덮었느느 식으로 정책 평가를 진행한다. \n",
    "\n",
    "env_state_after_action(state, action) = 행동을 취했을 경우에 다음 상태가 어딘지 알려주는 역할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####정책 평가\n",
    "    def policy_evaluation(self):\n",
    "        #next_value table 다음 가치함수 초기화\n",
    "        next_value_table = [[0.00] * self.env.width\n",
    "                                    for _ in range(self.env.height)]\n",
    "\n",
    "        # 모든 상태에 대해 벨만 기대 방정식을 계산\n",
    "        for state in self.env.get_all_states():\n",
    "            value = 0.0\n",
    "            # 마침 상태(terminal state)의 가치함수 = 0 \n",
    "            if state == [2, 2]:\n",
    "                next_value_table[state[0]][state[1]] = value\n",
    "                continue\n",
    "            #벨만 기대 방정식\n",
    "            for action in self.env.possible_actions:\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                value += (self.get_policy(state)[action] *\n",
    "                          (reward + self.discount_factor * next_value))\n",
    "\n",
    "            next_value_table[state[0]][state[1]] = round(value, 2)\n",
    "\n",
    "        self.value_table = next_value_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벨만 기대 방정식을 계산하는 부분\n",
    "- 정책이 각 행동에 대한 확률을 나타내기 때문에 모든 행동에 대해 value를 계산하고 더하면 기댓값을 계산한 것이 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value += (self.get_policy(state)[action] *\n",
    "                          (reward + self.discount_factor * next_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### policy_improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정책 평가를 통해 정책을 평가하면 그에 따른 새로운 가치함수를 얻는다. 에이젼트는 새로운 가치함수를 통해 정책을 업데이트한다. 정책 평가에서와 마찬가지로 정책 발전에서 정책 policy_table을 복사한 next_policy에 업데이트된 정책을 저장한다.  정책을 업데이트하는 방법 중에서 탐욕 정책 발전을 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####정책 발전\n",
    "    def policy_improvement(self):\n",
    "        next_policy = self.policy_table\n",
    "        #### 모든 상태에 대해 정책 발전\n",
    "        for state in self.env.get_all_states():\n",
    "            if state == [2, 2]:\n",
    "                continue\n",
    "            value = -99999\n",
    "            max_index = []\n",
    "            #### 반환할 정책 초기화\n",
    "            result = [0.0, 0.0, 0.0, 0.0]  # initialize the policy\n",
    "\n",
    "            # for every actions, calculate\n",
    "            #### 모든 행동에 대해 [보상 + (감가율 *다음 상태 가치함수)] 계산\n",
    "            for index, action in enumerate(self.env.possible_actions):\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                temp = reward + self.discount_factor * next_value\n",
    "\n",
    "                # We normally can't pick multiple actions in greedy policy.\n",
    "                # but here we allow multiple actions with same max values\n",
    "                #### 받을 보상이 최대인 행동의 인덱스(최대가 복수라면 모두)를 추출\n",
    "                if temp == value:\n",
    "                    max_index.append(index)\n",
    "                elif temp > value:\n",
    "                    value = temp\n",
    "                    max_index.clear()\n",
    "                    max_index.append(index)\n",
    "\n",
    "            #### 행동의 확률 계산\n",
    "            prob = 1 / len(max_index)\n",
    "\n",
    "            for index in max_index:\n",
    "                result[index] = prob\n",
    "\n",
    "            next_policy[state[0]][state[1]] = result\n",
    "\n",
    "        self.policy_table = next_policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "탐욕 정책을 구하는 순서\n",
    "\n",
    "1. 현재 상태에서 가능한 행동에 대해 R + &gamma;v(s')를 계산한다\n",
    "2. 계산한 값 중에서 가장 높은 값의 인덱스를 추려내 max_index 리스트에 담는다. \n",
    "\n",
    "R + &gamma;v(s')의 값이 같을 경우 max_index에 여러 값이 담길 것이다. 담긴 값이 여러 개라면 에이젼트는 해당하는 행동을 동일한 확률로 선택한다. 이를 구현하기 위해 1을 max_index 길이로 나눠서 최대 보상을 받게하는 행동의 확률로 저장한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # for every actions, calculate\n",
    "            #### 모든 행동에 대해 [보상 + (감가율 *다음 상태 가치함수)] 계산\n",
    "            for index, action in enumerate(self.env.possible_actions):\n",
    "                next_state = self.env.state_after_action(state, action)\n",
    "                reward = self.env.get_reward(state, action)\n",
    "                next_value = self.get_value(next_state)\n",
    "                temp = reward + self.discount_factor * next_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We normally can't pick multiple actions in greedy policy.\n",
    "                # but here we allow multiple actions with same max values\n",
    "                #### 받을 보상이 최대인 행동의 인덱스(최대가 복수라면 모두)를 추출\n",
    "                if temp == value:\n",
    "                    max_index.append(index)\n",
    "                elif temp > value:\n",
    "                    value = temp\n",
    "                    max_index.clear()\n",
    "                    max_index.append(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            #### 행동의 확률 계산\n",
    "            prob = 1 / len(max_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 후 [0.0,0.0,0.0,0.0]으로 초기화했던 result에 max_index 리스트에서 최대의 보상을 받게 하는 행동의 인덱스에 위에서 구한 확률을 넣는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            for index in max_index:\n",
    "                result[index] = prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_action(self, state):\n",
    "        random_pick = random.randrange(100) / 100\n",
    "\n",
    "        policy = self.get_policy(state)\n",
    "        policy_sum = 0.0\n",
    "        # return the action in the index\n",
    "        for index, value in enumerate(policy):\n",
    "            policy_sum += value\n",
    "            if random_pick < policy_sum:\n",
    "                return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용자는 정책 평가와 정책 발전을 통해 얻은 정책에 따라 에이젼트를 Move라는 버튼을 눌러서 움직일 수 있다. 에이젼트가 정책에 따라서 움직이려면 특정 상태에서 어떤 행동을 해야할지 알아야하고, 그 역할을 하는 것이 **get_action** 함수이다. \n",
    "\n",
    "정책은 대부분의 경우 하나의 행동에 대해 1의 확률을 가진다. 하지만 어떤 상태에서는 여러 개의 행동이 동일한 확률을 가질 수도 있다. 이때는 0~1 사이의 값을 하나 무작위로 추출하고 정책의 확률을 차례대로 더해가며 무작위로 추출한 수를 넘게 하는 행동을 찾아낸다. 그러면 에이젼트는 해당 상태에 대해 어떤 행동을 해야하는 지 알게 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_policy, get_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태에 따른 정책 반환\n",
    "    def get_policy(self, state):\n",
    "        if state == [2, 2]:\n",
    "            return 0.0\n",
    "        return self.policy_table[state[0]][state[1]]\n",
    "# 가치 함수의 값을 반환\n",
    "    def get_value(self, state):\n",
    "        #소수점 둘째 자리까지만 계산\n",
    "        return round(self.value_table[state[0]][state[1]], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_policy의 경우 self.policy_table로 저장돼 있는 정책에서 해당 상태에 대한 정책을 반환한다. get_value의 경우 set.value_table로 저장돼 있는 가치함수에서 해당 상태에 해당하는 가치함수를 반환하는데, 화면에 보여주기 위해서 소수 둘째자리만 표시한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정책 이터레이션 코드 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래 에이젼트는 정책 평가와 발전을 자동으로 번갈아 반복하지만, 여기 그리드월드 예제에서는 사용자가 버튼을 눌러서 정책 평가와 발전을 실행하게 했음. \n",
    "\n",
    "**정책 평가를 몇 번 하든 Evaluation 버튼과 Improvement 버튼을 번갈아 누른다면 에이젼트는 최적 정책을 찾아낸다 **\n",
    "\n",
    "**한 번 Evaluation 버튼을 누른 화면은 각 상태의 가치함수를 업데이트한다.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
