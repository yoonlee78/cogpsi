{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4. Sequence models and literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 유니버스 / CogΨ : RL & NLP - NLP 기초 [1]\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 차례\n",
    "1. A conversation with Andrew Ng\n",
    "2. Introduction\n",
    "3. Looking into the code\n",
    "4. Training the data\n",
    "5. More on training the data\n",
    "6. Notebook for lesson 1\n",
    "7. Finding what the next word should be\n",
    "8. Example\n",
    "9. Predicting a word\n",
    "10. Poetry!\n",
    "11. Looking into the code\n",
    "12. Laurence the poet!\n",
    "13. Your next task\n",
    "* Week 4 Quiz\n",
    "* Exercise 4- Using LSTMs, see if you can write Shakespeare!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we thought it might be a bit of fun to turn the tables away from classification and use your knowledge for prediction. \n",
    "* Given a body of words, you could conceivably predict the word most likely to follow a given word or phrase, and once you've done that, to do it again, and again. \n",
    "* With that in mind, this week you'll build a poetry generator. \n",
    "* It's trained with the lyrics from traditional Irish songs, and can be used to produce beautiful-sounding verse of it's own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고 [2]\n",
    "    - Instructor의 colab - https://colab.research.google.com/github/lmoroney"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A conversation with Andrew Ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One of the most fun applications of sequence models, is that they can read the body of text, so train on the certain body of text, and then generate or synthesize new texts, that sounds like it was written by similar author or set of authors. \n",
    "* In the courses, we're going to take a body of work from Shakespeare, and Shakespeare as a medieval English author, and so he wrote in a different style of English than we're used to reading, and it makes for a really interesting exercise in text generation,\n",
    "  - because if you're not familiar with like Shakespearean language and how it is done, then the language is actually generated by the neural network, will probably look a lot like the original one, probably, if you lived in the 1600's when Shakespeare was around, you'd be able to identify as being generated by a neural network, but for us now with this slightly different version of England, it actually makes for a really fun scenario. \n",
    "* There's a really fun application in neural network, and one of my favorite teachers in high school, was actually my English teacher that made me memorize a lot of Shakespeare. I really wonder what she would think of this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We've seen classification of text over the last few lessons. \n",
    "* But what about if we want to generate new text. \n",
    "* Instead of generating new text, how about thinking about it as a prediction problem. \n",
    "    - Remember when for example you had a bunch of pixels for a picture, and you trained a neural network to classify what those pixels were, and it would predict the contents of the image, \n",
    "    - like maybe a fashion item, or a piece of handwriting. \n",
    "    - Well, text prediction is very similar. \n",
    "* We can get a body of texts, \n",
    "    - extract the full vocabulary from it, \n",
    "    - and then create datasets from that, \n",
    "        - where \n",
    "            - we make it phrase the Xs \n",
    "            - and the next word in that phrase to be the Ys. \n",
    "* For example, consider the phrase, Twinkle, Twinkle, Little, Star. \n",
    "    - What if we were to create training data \n",
    "        - where \n",
    "            - the Xs are Twinkle, Twinkle, Little, \n",
    "            - and the Y is star. \n",
    "    - Then, whenever neural network \n",
    "        - sees the words Twinkle, Twinkle, Little, \n",
    "        - the predicted next word would be star. \n",
    "* Thus given enough words in a corpus with a neural network trained on each of the phrases in that corpus, and the predicted next word, we can come up with some pretty sophisticated text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Looking into the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So let's start with a simple example. \n",
    "* I've taken a traditional Irish song and here's the first few words of it\n",
    "* In this case to keep things simple, I put the entire song into a single string. \n",
    "* You can see that string here and I've denoted line breaks with \\n. \n",
    "* I can create a Python list of sentences from the data \n",
    "* Using the tokenizer, \n",
    "    - it will create the dictionary of words \n",
    "    - and the overall corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap03_1.png\" />\n",
    "<img src=\"figures/cap03_2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So now, let's look at the code \n",
    "    - to take this corpus and \n",
    "    - turn it into training data. \n",
    "* Here's the beginning, I will unpack this line by line. \n",
    "    - First of all, our training x's will be called input sequences\n",
    "    - and this will be a Python list. \n",
    "* Then for each line in the corpus, we'll generate a token list using the tokenizers\n",
    "    - This will convert \n",
    "        - a line of text like, \n",
    "            - \"In the he town of Athy one Jeremy Lanigan,\" \n",
    "        - into \n",
    "            - a list of the tokens representing the words. \n",
    "* Then we'll iterate over this list of tokens and create a number of n-grams sequences\n",
    "    - namely \n",
    "        - the first two words in the sentence or one sequence, \n",
    "        - then the first three are another sequence etc. \n",
    "    - The result of this will be, for the first line in the song, the following input sequences that will be generated. \n",
    "    - The same process will happen for each line, \n",
    "        - but as you can see, the input sequences are \n",
    "            - simply the sentences being broken down into \n",
    "                - phrases, \n",
    "                - the first two words, \n",
    "                - the first three words, etc. \n",
    "* We next need to find the length of the longest sentence in the corpus. \n",
    "* Once we have our longest sequence length, the next thing to do is pad all of the sequences so that they are the same length. \n",
    "    - We will pre-pad with zeros to make it easier to extract the label, you'll see that in a few moments. \n",
    "* Now, that we have our sequences, the next thing we need to do is turn them into x's and y's, our input values and their labels. \n",
    "    - When you think about it, now that the sentences are represented in this way, all we have to do is take all but the last character as the x and then use the last character as the y on our label. \n",
    "* By this point, it should be clear why we did pre-padding, \n",
    "    - because it makes it much easier for us to get the label simply by grabbing the last token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap04_1.png\" />\n",
    "<img src=\"figures/cap04_2.png\" />\n",
    "<img src=\"figures/cap04_3.png\" />\n",
    "<img src=\"figures/cap04_4.png\" />\n",
    "<img src=\"figures/cap04_5.png\" />\n",
    "<img src=\"figures/cap04_6.png\" />\n",
    "<img src=\"figures/cap04_7.png\" />\n",
    "<img src=\"figures/cap04_8.png\" />\n",
    "<img src=\"figures/cap04_9.png\" />\n",
    "<img src=\"figures/cap04_10.png\" />\n",
    "<img src=\"figures/cap04_11.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. More on training the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So now, we have to split our sequences into our x's and our y's. \n",
    "* So to get my x's, I just get all of the input sequences sliced to remove the last token. \n",
    "* To get the labels, I get all of the input sequence sliced to keep the last token. \n",
    "* Now, I should one-hot encode my labels as this really is a classification problem. \n",
    "    - Where given a sequence of words, I can classify from the corpus, what the next word would likely be. \n",
    "    - So to one-hot encode, I can use the contrast utility to convert a list to a categorical.\n",
    "    - I simply give it the list of labels and the number of classes which is my number of words, and it will create a one-hot encoding of the labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap05_1.png\" />\n",
    "<img src=\"figures/cap05_2.png\" />\n",
    "<img src=\"figures/cap05_3.png\" />\n",
    "<img src=\"figures/cap05_4.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Notebook for lesson 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 실습 링크 -  [lesson 1 notebook ](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%204%20-%20Lesson%201%20-%20Notebook.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Finding what the next word should be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the previous video we looked at the data, a string containing a single song, and saw how to prepare that for generating new text. \n",
    "    - We saw how to tokenize the data and then create sub-sentence engrams that were labelled with the next word in the sentence. \n",
    "    - We then one-hot encoded the labels to get us into a position where we can build a neural network that can, given a sentence, predict the next word. \n",
    "* Now that we have our data as xs and ys, it's relatively simple for us to create a neural network to classify what the next word should be, given a set of words. \n",
    "* Here's the code. \n",
    "    - We'll start with an embedding layer. \n",
    "        - We'll want it to handle all of our words, so we set that in the first parameter. \n",
    "        - The second parameter is the number of dimensions to use to plot the vector for a word. \n",
    "            - Feel free to tweak this to see what its impact would be on results, \n",
    "            - but I'm going to keep it at 64 for now. \n",
    "            - Finally, the size of the input dimensions will be fed in, and this is the length of the longest sequence minus 1. \n",
    "                - We subtract one because we cropped off the last word of each sequence to get the label, \n",
    "                - so our sequences will be one less than the maximum sequence length. \n",
    "    - Next we'll add an LSTM. As we saw with LSTMs earlier in the course, \n",
    "        - their cell state means that they carry context along with them, \n",
    "            - so it's not just next door neighbor words that have an impact. \n",
    "        - I'll specify 20 units here, but again, you should feel free to experiment. \n",
    "    - Finally there's a dense layer sized as the total words, \n",
    "        - which is the same size that we used for the one-hot encoding. \n",
    "        - Thus this layer will have one neuron, per word and \n",
    "            - that neuron should light up when we predict a given word. \n",
    "    - We're doing a categorical classification, so we'll set the laws to be categorical cross entropy. \n",
    "    - And we'll use the atom optimizer, which seems to work particularly well for tasks like this one. \n",
    "    - Finally, we'll train for a lot of epoch, say about 500, as it takes a while for a model like this to converge, particularly as it has very little data. \n",
    "        - So if we train the model for 500 epochs, it will look like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap07_1.png\" />\n",
    "<img src=\"figures/cap07_2.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here are a few phrases that were generated \n",
    "    - when I gave the neural network the sentence \n",
    "        - \"Lawrence went to Dublin\", \n",
    "        - and I asked it to predict \n",
    "            - the next 10 words. \n",
    "* notice that there's a lot of repetition of words. \n",
    "* This is because our LSTM was only carrying context forward. \n",
    "* Let's take a look at what happens if we change the code to be bidirectional. \n",
    "    - I can see that I do converge a bit quicker as you'll see in this chart. \n",
    "* They make a little bit more sense, but there's still some repetition. \n",
    "    - That being said, remember this is a song where words rhyme such as ball, all and wall, et cetera, and as such many of them are going to show up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap08_1.png\" />\n",
    "<img src=\"figures/cap08_2.png\" />\n",
    "<img src=\"figures/cap08_3.png\" />\n",
    "<img src=\"figures/cap08_4.png\" />\n",
    "<img src=\"figures/cap08_5.png\" />\n",
    "<img src=\"figures/cap08_6.png\" />\n",
    "<img src=\"figures/cap08_7.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Predicting a word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So now, let's take a look at how to get a prediction for a word and how to generate new text based on those predictions. \n",
    "* So let's start with a single sentence. \n",
    "    - For example, 'Lawrence went to Dublin.' \n",
    "        - I'm calling this sentence the seed. \n",
    "* If I want to predict the next 10 words in the sentence to follow this, \n",
    "    - then this code will tokenizer that for me using the text to sequences method on the tokenizer. \n",
    "    - As we don't have an outer vocabulary word, it will ignore 'Lawrence,' which isn't in the corpus and will get the following sequence. \n",
    "* This code will then pad the sequence so it matches the ones in the training set. \n",
    "* So we end up with something like this which we can pass to the model to get a prediction back. \n",
    "* This will give us the token of the word most likely to be the next one in the sequence. \n",
    "* So now, we can do a reverse lookup on the word index items to turn the token back into a word and to add that to our seed texts, and that's it. Here's the complete code to do that 10 times and you can tweak it for more. \n",
    "* But do you know that the more words you predict, the more likely you are going to get gibberish? \n",
    "    - Because each word is predicted, so it's not 100 per cent certain, and then the next one is less certain, and the next one, etc. \n",
    "    - So for example, if you try the same seed and predict 100 words, you'll end up with something like this. \n",
    "* Using a larger corpus we'll help, and then the next video, you'll see the impact of that, as well as some tweaks that a neural network that will help you create poetry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap09_1.png\" />\n",
    "<img src=\"figures/cap09_2.png\" />\n",
    "<img src=\"figures/cap09_3.png\" />\n",
    "<img src=\"figures/cap09_4.png\" />\n",
    "<img src=\"figures/cap09_5.png\" />\n",
    "<img src=\"figures/cap09_6.png\" />\n",
    "<img src=\"figures/cap09_7.png\" />\n",
    "<img src=\"figures/cap09_8.png\" />\n",
    "<img src=\"figures/cap09_10.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Poetry!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the previous video, \n",
    "    - we used the single song to generate text. \n",
    "    - We got some text from it but once we tried to predict beyond a few words, \n",
    "    - it rapidly became gibberish. \n",
    "* So in this video, \n",
    "    - we'll take a look at adapting that work for a larger body of words to see the impact.\n",
    "* The good news is that it will require very little code changes, so you'll be able to get it working quite quickly. \n",
    "* I've prepared a file \n",
    "    - with a lot of songs that has 1,692 sentences in all to see what the impact would be on the poetry that a neural network would create. \n",
    "* To download these lyrics, you can use this code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 다운로드 - [irish-lyrics](https://storage.googleapis.com/laurencemoroney-blog.appspot.com/irish-lyrics-eof.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap10_1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Looking into the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now instead of hard-coding the song into a string called data, I can read it from the file like this. \n",
    "* I've updated the model a little bit to make it work better with a larger corpus of work \n",
    "    - but please feel free to experiment with these hyper-parameters. \n",
    "        - Three things that you can experiment with. \n",
    "            - First, is the dimensionality of the embedding, \n",
    "                - 100 is purely arbitrary and I'd love to hear what type of results you will get with different values.\n",
    "            - Similarly, I increase the number of LSTN units to 150. \n",
    "                - Again, you can try different values \n",
    "                - or you can see how it behaves if you remove the bidirectional. \n",
    "                    - Perhaps you want words only to have forward meaning, where big dog makes sense but dog big doesn't make so much sense. \n",
    "            - Perhaps the biggest impact is on the optimizer. \n",
    "                - Instead of just hard coding Adam as my optimizer this time and getting the defaults, \n",
    "                    - I've now created my own Adam optimizer and set the learning rate on it.\n",
    "                    - Try experimenting with different values here and see the impact that they have on convergence. \n",
    "                    - In particular, see how different convergences can create different poetry. \n",
    "* And of course, training for different epochs will always have an impact with more generally being better but eventually you'll hit the law of diminishing returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap11_1.png\" />\n",
    "<img src=\"figures/cap11_2.png\" />\n",
    "<img src=\"figures/cap11_3.png\" />\n",
    "<img src=\"figures/cap11_4.png\" />\n",
    "<img src=\"figures/cap11_5.png\" />\n",
    "<img src=\"figures/cap11_6.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Laurence the poet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In a co-lab with this data and these parameters, using a GPU, it typically takes about 20 minutes to train a model. \n",
    "* Once it's done, try a a seed sentence and get it to give you 100 words. \n",
    "* Note that there are no line breaks in the prediction, so you'll have to add them manually to turn the word stream into poetry. \n",
    "* Here's a simple example. \n",
    "    - I used a famous quote from a very famous movie and let's see if you can recognize it.\n",
    "    - And I tried that to see what type of poem it would give me and I got this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cap12_1.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Your next task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, this approach works very well until you have very large bodies of text with many many words. \n",
    "* So for example, \n",
    "    - you could try the complete works of Shakespeare \n",
    "    - and you'll likely hit memory errors, \n",
    "        - as assigning the one-hot encodings of the labels to matrices that have over 31,477 elements, \n",
    "            - which is the number of unique words in the collection, \n",
    "        - and there are over 15 million sequences generated using the algorithm that we showed here. \n",
    "        - So the labels alone would require the storage of many terabytes of RAM. \n",
    "* So for your next task, you'll go through a workbook by yourself that uses character-based prediction. \n",
    "    - The full number of unique characters in a corpus is far less than the full number of unique words, at least in English. \n",
    "    - So the same principles that you use to predict words can be used to apply here. \n",
    "    - The workbook is at this URL, so try it out, and once you've done, that you'll be ready for this week's final exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://www.tensorflow.org/tutorials/sequences/text_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4 Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Quiz 링크](https://www.coursera.org/learn/natural-language-processing-tensorflow/exam/Vxr03/week-4-quiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 4 Outro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Over the last four weeks you've gotten a grounding in how to do Natural Language processing with TensorFlow and Keras. \n",
    "    - You went from first principles -- basic Tokenization and Padding of text to produce data structures that could be used in a Neural Network.\n",
    "* You then learned about embeddings, \n",
    "* and how words could be mapped to vectors, \n",
    "* and words of similar semantics given vectors pointing in a similar direction, \n",
    "    - giving you a mathematical model for their meaning, which could then be fed into a deep neural network for classification.\n",
    "* From there you started learning about sequence models, \n",
    "* and how they help deepen your understanding of sentiment in text \n",
    "    - by not just looking at words in isolation, \n",
    "    - but also how their meanings change when they qualify one another.\n",
    "* You wrapped up by taking everything you learned and using it to build a poetry generator!\n",
    "* This is just a beginning in using TensorFlow for natural language processing. \n",
    "* I hope it was a good start for you, and you feel equipped to go to the next level!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4- Using LSTMs, see if you can write Shakespeare!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this course you’ve done a lot of NLP and text processing. \n",
    "* This week you trained with a dataset of Irish songs to create traditional-sounding poetry.\n",
    "* For this week’s exercise, you’ll take a corpus of Shakespeare sonnets, \n",
    "* and use them to train a model. \n",
    "* Then, see if that model can create poetry!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [실습 colab 노트북](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/NLP_Week4_Exercise_Shakespeare_Question.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [실습 정답 colab 노트북](https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/NLP_Week4_Exercise_Shakespeare_Answer.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고자료 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [1] Natural Language Processing in TensorFlow - https://www.coursera.org/learn/natural-language-processing-tensorflow\n",
    "* [2] Instructor의 colab - https://colab.research.google.com/github/lmoroney"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
